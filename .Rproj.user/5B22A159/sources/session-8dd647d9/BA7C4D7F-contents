% devtools::install_github("oonyambu/UniPro")

\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{subcaption}
%\usepackage{ulem} % this affects reference styles
\usepackage[margin=2cm]{geometry}
%\usepackage[style=authoryear, sorting=nyt, backend=bibtex, sortcites=true]{biblatex}
\usepackage[style=authoryear, backend=bibtex, sortcites=true]{biblatex}
%\addbibresource{../../include/reference.bib}
\addbibresource{reference-DE.bib}

\usepackage{fullpage}
\renewcommand{\baselinestretch}{1.25}% {1.25}
\renewcommand{\arraystretch}{1.1}
\renewcommand{\tabcolsep}{4pt}

%% trace revision using color
\newcommand{\reva}[1]{{\color{red} #1}}
\newcommand{\revb}[1]{{\color{blue} #1}}
\newcommand{\revc}[1]{{\color{cyan} #1}}
\newcommand{\revd}[1]{{\color{brown} #1}}
\newcommand{\reve}[1]{{\color{blue} #1}}
%% to remove color, uncomment the following
%\renewcommand{\reva}[1]{{#1}}  % remove color
%\renewcommand{\revb}[1]{{#1}}
%\renewcommand{\revc}[1]{{#1}}  % remove color
%\renewcommand{\revd}[1]{{#1}}
%\renewcommand{\reve}[1]{{#1}}

\usepackage{amsmath,amssymb}
\newcommand{\X}{\boldsymbol{X}}
\newcommand{\Y}{\boldsymbol{Y}}
\newcommand{\x}{\boldsymbol{x}}
%\renewcommand{\u}{\boldsymbol{u}}
\newcommand{\y}{\boldsymbol{y}}
\newcommand{\z}{\boldsymbol{z}}
\newcommand{\w}{\boldsymbol{w}}
\newcommand{\R}{\boldsymbol{R}}
\newcommand{\Rinv}{\boldsymbol{R}^{-1}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\cov}{\textrm{Cov}}

%\title{\bf A Study of Hyperparameter Tuning in a Differential Evolution Algorithm for Constructing Uniform Projection Designs}
\title{\bf Tuning Differential Evolution Algorithm for Constructing Uniform Projection Designs}
\author{Samuel Onyambu and Hongquan Xu \\ Department of Statistics and Data Science, \\ University of California, Los Angeles, CA 90095, USA}
%\date{}
\usepackage{Sweave}
\begin{document}

\maketitle
\begin{abstract}
%\reva{Space-filling designs are widely used in computer experiments to study complex systems. Uniform projection designs are specialized space-filling designs with attractive low-dimensional propection properties and robustness to other criteria. However, there is no available algorithm for efficiently generating such designs. We consider the construction of uniform projection designs via a differential evolution algorithm. Differential evolution  belongs to the class of evolutionary algorithms and has gained popularity for its simplicity, robustness, and effectiveness in solving complex optimization problems. Yet, its performance heavily depends on several hyperparameters.  We aim at understanding the surface structure of the involved hyperparameters, the importance contribution of each hyperparameter, and thereby provide a guideline on optimal hyperparameter settings under different setups. We take a comprehensive study to compare different types of experimental designs and surrogate models for this purpose.}\\
%Differential Evolution (DE) is a powerful optimization algorithm inspired by the principles of natural evolution. It belongs to the class of evolutionary algorithms and has gained popularity for its simplicity, robustness, and effectiveness in solving complex optimization problems. As it works with continuous data, it has to be modified in order to work with discrete data such as design generation. Recently, \textcite{stokes2024metaheuristic} proposed such a modified DE algorithm, yet its performance heavily depends on several hyperparameters.  We aim at understanding the surface structure of the involved hyperparameters, the importance contribution of each hyperparameter, and thereby provide a guideline on optimal hyperparameter settings under different setups. We compare different types of experimental designs and surrogate models for this purpose. We illustrate the method for constructing uniform projection designs.
Space-filling designs are extensively used in computer experiments to analyze complex systems. Among these, uniform projection designs stand out for their desirable low-dimensional projection properties and robustness against other criteria. However, no efficient algorithm currently exists for generating such designs. This study explores the construction of uniform projection designs using a differential evolution (DE) algorithm. DE, an evolutionary algorithm, is known for its simplicity, robustness, and effectiveness in solving complex optimization problems, though its performance is highly sensitive to several hyperparameters. Our goal is to investigate the structure of the hyperparameter space, evaluate the contribution of each hyperparameter, and provide guidelines for optimal hyperparameter settings across various scenarios. To achieve this, we conduct a comprehensive comparison of different experimental designs and surrogate models.
\end{abstract}

\begin{quotation} {\em Keywords:} Computer experiment, experimental design, hyperparameter optimization, kriging model, metaheuristic algorithm, space-filling design.
\end{quotation}

\newpage
\section{Introduction}



Experimental design construction is a fundamental aspect of research and data-driven inquiry, aimed at organizing experimental runs to extract maximum information while minimizing resource use. By strategically selecting input combinations, well-constructed designs ensure that researchers can identify key factors, estimate model parameters, and predict responses accurately \parencite{montgomery2017design}. The primary goal is to balance efficiency and comprehensiveness, whether in exploring high-dimensional spaces, optimizing processes, or assessing system robustness. Central to this process is the notion of space-filling, where design points are distributed to capture variations across the entire experimental domain, providing a solid foundation for modeling and inference \parencite{santner2018design}.

%\reva{Computer experiments have become ubiquitous across the engineering disciplines and throughout the physical sciences over the last few decades.}
%Space-filling designs are crucial in computer experiments, as they help avoid over-sampling in certain regions and under-sampling in others, ensuring \reva{an accurate} representation of the entire input space \parencite{kamath2022intelligent}. \reva{A space-filling design generally refers to any design that spreads its points {throughout} the design domain in some uniform fashion. The uniformity of a design may be evaluated in terms of distance \parencite{johnson1990minimax}, orthogonality \parencite{owen1994controlling}, or discrepancy \parencite{fang2000uniform}.}
%\textcite{joseph2016space} \reva{gave} an overview as to how space-filling designs are used to overcome optimization problems involving computationally intensive computer models. By reducing model error and improving predictive performance, these designs enhance the reliability of simulation models, making them essential in applications such as kriging and optimization \parencite{joseph2019space}.
%\reva{For a comprehensive introduction to computer experiments and space-filling designs, see  \textcite{fang2006design} and \textcite{santner2018design}. For some recent developments, see, among many others, \textcite{yin2023construction, tian2024stratification, shi2024projection, li2024construction, wang2024construction, chen2024selecting, vazquez2024integer, yuan2025construction}.}

{Over the past few decades, computer experiments have become ubiquitous across engineering disciplines and the physical sciences. Space-filling designs play a fundamental role in these experiments, as they ensure a well-balanced exploration of the input space, preventing excessive sampling in certain regions while avoiding sparse coverage in others \parencite{kamath2022intelligent}. Broadly speaking, a space-filling design refers to any design that distributes its points uniformly across the design domain. The uniformity of a design can be assessed based on criteria such as distance \parencite{johnson1990minimax}, orthogonality \parencite{owen1994controlling}, or discrepancy \parencite{fang2000uniform}.

\textcite{joseph2016space} provided a comprehensive overview of how space-filling designs help address optimization challenges associated with computationally demanding computer models. By minimizing model error and enhancing predictive accuracy, these designs significantly improve the reliability of simulation models, making them indispensable in applications such as kriging and optimization \parencite{joseph2019space}.
For an in-depth introduction to computer experiments and space-filling designs, see \textcite{fang2006design} and \textcite{santner2018design}. For recent advancements in this field, refer to \textcite{yin2023construction, tian2024stratification, shi2024projection, li2024construction, wang2024construction, chen2024selecting, vazquez2024integer, yuan2025construction}, among others.}


 %In the quest for good space-filling designs,
 %uniform projection designs (UPDs) have emerged as a powerful tool for ensuring uniformity across all low-dimensional projections of the design space \parencite{sun2019uniform}. UPDs, introduced by \textcite{sun2019uniform}, are specialized space-filling designs characterized by robust performance across various design criteria and impressive space-filling properties in multiple dimensions. These designs are particularly valuable in high-dimensional settings where relationships between subsets of factors often carry critical information. \textcite{wang2022design} studied the connection between UPDs and maximin distance designs and developed novel lower and upper bounds for UPDs.  \textcite{sun2023uniform} studied the connection between UPDs and strong orthogonal arrays of strength 2+ and showed that strong orthogonal arrays of strength 2+ are optimal or nearly optimal under the uniform projection criterion. Despite of these developments, existing algorithms for generating UPDs remain underexplored, highlighting an important area for further research. %To construct UPDs, we leverage the use of Differential Evolution (DE).
%To fill this gap, we explore the use of Differential Evolution (DE), a powerful metaheuristic algorithm, for constructing UPDs.

In the pursuit of high-quality space-filling designs, uniform projection designs (UPDs) have emerged as a powerful approach for achieving uniformity across all low-dimensional projections of the design space \parencite{sun2019uniform}. Introduced by \textcite{sun2019uniform}, UPDs are a specialized class of space-filling designs that exhibit strong performance across various design criteria while maintaining excellent space-filling properties in multiple dimensions. These designs are particularly valuable in high-dimensional settings, where interactions among subsets of factors often hold critical importance.
\textcite{wang2022design} investigated the relationship between UPDs and maximin distance designs, establishing novel lower and upper bounds for UPDs. Additionally, \textcite{sun2023uniform} explored the connection between UPDs and strong orthogonal arrays of strength 2+, demonstrating that such arrays are either optimal or nearly optimal under the uniform projection criterion. Despite these advancements, existing algorithms for generating UPDs remain relatively underdeveloped, presenting a significant avenue for further research.
To address this gap, we explore the application of Differential Evolution (DE), a powerful metaheuristic optimization algorithm, for constructing UPDs.

%Metaheuristic algorithms such as simulated annealing algorithm, genetic algorithm, particle swarm optimization, and threshold accepting algorithm have been used in the construction of space-filling designs, such as maximin distance Latin hypercube designs \parencite{morris1995exploratory, liefvendahl2006study, chen2013optimizing}, uniform designs \parencite{fang2000uniform}, and maximum projection designs \parencite{joseph2015maximum, wang2024construction}.

%Recently, \textcite{stokes2024metaheuristic} proposed a novel DE algorithm for constructing order-of-addition designs, showcasing its efficiency compared to other metaheuristic algorithms. Inspired by their findings, we adapt and extend their DE algorithm for the construction of UPDs.

Metaheuristic algorithms, including simulated annealing, genetic algorithms, particle swarm optimization, and threshold accepting, have been employed in the construction of space-filling designs. These methods have been instrumental in generating designs such as maximin distance Latin hypercube designs \parencite{morris1995exploratory, liefvendahl2006study, chen2013optimizing}, uniform designs \parencite{fang2000uniform}, and maximum projection designs \parencite{joseph2015maximum, wang2024construction}.
Recently, \textcite{stokes2024metaheuristic} introduced a novel Differential Evolution (DE) algorithm for constructing order-of-addition designs, demonstrating its superior efficiency compared to other metaheuristic approaches. Building on their insights, we adapt and extend this DE algorithm to facilitate the construction of UPDs.

The performance of the DE algorithm is significantly influenced by its hyperparameters, which dictate the learning process of the optimization \parencite{price2006differential}. An inappropriate setting of these hyperparameters can lead to suboptimal performance of the DE algorithm. While the variant DE  algorithm proposed by \textcite{stokes2024metaheuristic} encompasses several hyperparameters, their effects remain largely unexamined. Therefore, we aim to conduct a comprehensive study of the hyperparameters to elucidate their impacts on the algorithm's performance, providing insights that could enhance its effectiveness.

%\revb{The challenge of determining the optimal hyperparameter settings for any learning process  \revc{is well known}. Two primary frameworks dominate this landscape: the model-based framework and the model-free framework. Model-based hyperparameter optimization focuses on tuning hyperparameters by approximating the true learning algorithm, while model-free methods approach the optimization problem without parametric assumptions. Relevant literature on model-based hyperparameter optimization includes works by \textcite{falkner2018bohb, hutter2011sequential}. In contrast, model-free frameworks encompass techniques such as manual search, grid search, random search, genetic algorithms, and orthogonal array tuning methods \parencite{liashchynskyi2019grid}}.

The challenge of identifying the optimal hyperparameter configurations for any learning process is widely recognized, and researchers have sought to address it through two primary approaches: the model-based and model-free frameworks. Model-based hyperparameter optimization refines hyperparameters by approximating the underlying learning algorithm, whereas model-free methods tackle the optimization problem without imposing parametric assumptions. Foundational contributions to model-based hyperparameter optimization include the works of \textcite{falkner2018bohb, hutter2011sequential}. In contrast, model-free approaches encompass a diverse array of techniques, such as manual search, grid search, random search, genetic algorithms, and orthogonal array tuning methods \parencite{liashchynskyi2019grid}.


We take an integrated approach based on experimental design and analysis.
Our approach leverages various types of factorial designs and space-filling designs to investigate the effectiveness of DE in constructing UPDs.  We also employ different types of surrogate models such as second-order models and kriging models to evaluate the performance of different designs, enabling us to visualize the response surface of the DE algorithm's hyperparameters. This framework comprises the data generation, modeling, and analysis procedures. The insights gained from our analysis provide a general guideline for optimal hyperparameter settings necessary for generating good UPDs. Specifically, we aim to address four  objectives: (i) identifying useful design types in understanding the surface structure extended by the DE hyperparameters, (ii) determining the most effective surrogate models, (iii) determining efficient hyperparameter settings, and (iv) developing an efficient algorithm for the construction of UPDs. While we focus on tuning the DE algorithm, this methodology is versatile and can be applied to hyperparameter tuning in other optimization algorithms.

Our approach is quite different from the \textcite{lujan2018design} method which used a $2^k$ factorial design with the response surface method (RSM) and ridge regression for screening to select the important factors in the data. While \textcite{lujan2018design} focused on factor screening and selection with the traditional RSM approach, we emphasize on the comparisons of different types of designs and surrogate models in approximating the surface structure of the DE algorithm.

%\revb{While the objective of hyperparameter optimization is to determine the optimal combination of hyperparameters that maximizes a learning algorithm's performance, this paper also investigates how different design configurations affect the efficiency of hyperparameter tuning. The insights gained from this analysis are used to identify the best hyperparameters, which are then utilized by the DE algorithm to generate UPDs. This methodology is versatile and can be applied to hyperparameter tuning in other optimization algorithms.} % Repeating several sentences

The paper is organized as follows. In Section 2, we review the DE algorithm proposed by \textcite{stokes2024metaheuristic} and its hyperparameters. We describe various experimental designs and surrogate models used in the study, respectively, in Sections 3 and 4. In Section 5, we describe the uniform projection criterion proposed by \textcite{sun2019uniform} and the data generating process. Section 6 presents the results and Section 7 addresses factor importance and optimal hyperparameter settings. Finally, Section 8 concludes the paper.

\section{Differential Evolution Algorithm}

Originating from the pioneering work of \textcite{storn1997differential}, DE has emerged as a powerful heuristic optimization algorithm. It is an example of metaheuristic algorithm which draws its inspiration from the mechanisms of biological evolution.

Metaheuristic algorithms are a class of optimization techniques designed to find optimal or near-optimal solutions to complex problems, particularly when traditional optimization methods are impractical or inefficient. These algorithms are generally inspired by natural or physical processes and do not guarantee an exact solution but are highly effective for solving large-scale, non-linear, multi-dimensional, and combinatorial problems. They are flexible, adaptable, and can be applied to a wide variety of optimization problems. They typically balance exploration and exploitation to achieve efficient convergence \parencite{weise2009global}.

Metaheuristic algorithms can be broadly classified into three categories based on their inspiration: (1) evolutionary algorithms, (2) swarm intelligence, and (3) physical phenomena-based algorithms. Evolutionary algorithms, such as genetic algorithms  and DE, simulate the process of natural selection to evolve solutions. These methods are especially effective in maintaining solution diversity through operators like mutation and crossover, which prevent premature convergence \parencite{storn1997differential}. Swarm intelligence techniques, such as particle swarm optimization  and ant colony optimization, mimic the collective behavior of groups in nature, such as bird flocks or ant colonies. These approaches excel in leveraging communication among agents to quickly explore vast solution spaces \parencite{dorigo2007ant}. Lastly, physical phenomena-based algorithms, such as simulated annealing, draw from thermodynamics principles, allowing for probabilistic escape from local optima to find better solutions \parencite{kirkpatrick1983optimization,aarts1989simulated}.

Among these, DE stands out due to its simplicity, efficiency, and robustness across diverse problem landscapes. Unlike other metaheuristics, DE uses differential mutations, which introduce solution diversity and improve exploration capabilities while maintaining computational efficiency. This makes DE particularly suited for handling non-linear, non-convex optimization problems. Furthermore, its ability to balance exploration and exploitation ensures convergence to high-quality solutions even in noisy or complex landscapes, a feature critical for problems with large and intricate design spaces \parencite{das2010differential, yang2020nature}. By combining the strengths of population-based search and vectorized mutation strategies, DE demonstrates its utility in applications ranging from engineering optimization to machine learning.

To simulate survival-of-the-fittest dynamics, DE treats each candidate or agent as a chromosome made up of several genes and implements mutation and crossover procedures that allow beneficial genes to persist into future generations \parencite{storn1997differential}. DE operates on the principle of population-based search, where a set of candidate solutions evolves over successive generations towards optimal or near-optimal solutions. At its core, DE employs mutation, crossover, and selection operators to iteratively improve the quality of solutions. The algorithm's efficacy stems from its robustness, simplicity, and ability to handle non-linear, non-convex, and noisy optimization landscapes.


Without loss of generality, we assume that we want to minimize a real-valued objective function $\phi$ over an $m$-dimension space $\Omega$. It has five steps

\begin{enumerate}
    \item \textbf{Genetic Representation}:
    Let $\pi_1, \ldots ,\pi_N$ be the initial population, where each agent $\pi_i = (\pi_{i1},\dots,\pi_{im})$ is randomly chosen from $\Omega$.
    \item \textbf{Mutation}:
    Mutation expands the search space of the current population.
    For each $i = 1, \ldots , N$, mutation produces a potential donor $\nu_i$ in $\Omega$ by adding the weighted difference of two agents to a third,  all randomly chosen and distinct from the target $(\pi_i)$, that is,
    \begin{equation}
        \nu_i = \pi_a + w(\pi_b - \pi_c)\\
        \label{mutation}
    \end{equation}
    where $a \neq b \neq c$ are randomly chosen three distinct numbers from $1,\dots, N$, and they are all different from $i$.
    \item \textbf{Crossover}:
    Crossover blends the current generation of agents with the population of potential donors in order to form candidates for the next generation known as trial agents. For each $i = 1,\dots, N$, one of the $m$ variables of $\nu_i$ is randomly selected to directly enter the trial agent $u_i$. In this way, one variable is forced to change so that each $u_i$ will
    certainly differ from its original target $\pi_i$. Next, with probability $pCR$, more variables are taken from $u_i$ and placed in the trial agent. Whichever variables do not take their value from the donor inherit their original value from $\pi_i$. Assuming $j_0$ is a random number from $1, \dots, m$, this process can be written as follows: for $j = 1,\dots, m,$
    $$
    \begin{aligned}
    u_{ij} = \begin{cases} \nu_{ij}& \text{with probability }pCR
    \text{ or if } j= j_0,\\ \pi_{ij}& \text{otherwise}\end{cases}
    \end{aligned}
    $$
    \item \textbf{Selection}:
    Selection creates the next generation of agents by comparing each target to its respective trial agent. The trial agent is adopted if it leads to an improvement and is discarded otherwise. For minimization problems, this process is given by,
    $$
    \pi_i = \begin{cases} u_i & \text{if } \phi(u_i) < \phi(\pi_i)\\
    \pi_i &\text{otherwise}  \end{cases}
    $$
    \item \textbf{Repeat}:
    Repeat steps 2 through 4 over many generations until a specified
    stopping condition is satisfied.
\end{enumerate}

Though quite simplistic, its ability to balance exploration and exploitation makes it ideal for solving non-linear and multimodal problems.

Since experimental designs lie on a discrete and constrained space, we leverage the modified DE by \textcite{stokes2024metaheuristic}. This is to ensure that the resulting mutated design is feasible. Their proposed method modifies the mutation step by using the swap mutation  \parencite{michalewicz2013genetic}, one of the structural mutation operators in genetic algorithm.  In this operator, two positions in a solution are randomly selected, and their values are exchanged. This maintains the feasibility of the solution by preserving its permutation structure while introducing variation to explore the search space. The swap mutation is computationally efficient and effective at diversifying the population, reducing the risk of premature convergence. The mutation step is controlled by mutation rate $pMut$ which is the probability of swapping two elements. %In addition, borrowing from \reva{particle swarm optimization \parencite{ chen2013optimizing}, \textcite{stokes2024metaheuristic} induced the influence of the global best solution and the personal best solution with probability $pGBest$ and $pSelf$, respectively. The personal best solution is simply the current agent itself in the DE context.} This yielded an algorithm which contained the following hyperparameters:
{Additionally, inspired by particle swarm optimization \parencite{ chen2013optimizing}, \textcite{stokes2024metaheuristic} incorporated the influence of both the global best solution and the personal best solution, with probabilities $p{GBest}$ and $p{Self}$, respectively. In the context of DE, the personal best solution refers to the current agent itself. This resulted in an algorithm characterized by the following hyperparameters:}
\begin{itemize}
    \item $NP$ - The size of the population (N). % We consider values between $[10,  100]$
    \item $itermax$ -  the maximum number of iterations/generations used. % We consider values between $[500, 1500]$
    \item $pCR$ - Probability of crossover. %We consider values between $[0.05, 0.95]$
    \item $pMut$ - Probability of mutation. %We consider values between $[0.05, 0.95]$
    \item $pGBest$ - Probability of using the global best for mutation.
  %  We consider values between $[0.05, 0.95]$
    \item $pSelf$ - Probability of using the current agent for mutation.
\end{itemize}

%\textcite{stokes2024metaheuristic} proposed three different choices of the initial agent to be mutated to obtain the \reva{potential \revb{donor}}. This lead to three different variants \revc{which they referred to as DE1 which (revise)} uses the global best, DE2 which uses the current agent, and DE3 which uses a random agent.  \reva{In addition, they proposed a hybrid version which uses the global best, current agent and a random agent with probability 50\%, 25\% and 25\%, respectively. They referred to the hybrid version as DE4 and demonstrated that DE1 and DE4 are superior to DE2 and DE3.}

{\textcite{stokes2024metaheuristic} proposed three different choices for selecting the initial agent to be mutated in order to obtain the potential donor, leading to three distinct variants. Specifically, DE1 utilizes the global best agent, DE2 employs the current agent, and DE3 selects a random agent. Additionally, they introduced a hybrid version, DE4, which combines all three strategies, selecting the global best, current agent, and random agent with probabilities of 50\%, 25\%, and 25\%, respectively. Their findings demonstrated that DE1 and DE4 outperform DE2 and DE3.}

Regarding the hypermarameters, the first two, $NP$ and $itermax$, determine the budget size, whereas the other four hyperparameters affect the evolution process. The hyperparameters, $pGBest$ and $pSelf$, determine the probability of using the global best and the current agent in the mutation process, respectively. There is a constraint between these two hyperparameters, that is, $pGBest + pSelf \le 1$. The question that arises is how these hyperparameters interact with each other. Also whether we can do better than the proposed fixed probabilities to obtain  better settings for the DE algorithm for design generation.

In this study, we shall consider values between $[10,  100]$ for $NP$, $[500, 1500]$ for $itermax$, and $[0.05, 0.95]$ for $pCR$, $pMut$ and $pGBest$. We fix $pSelf = (1 - pGBest)/2$ so that there is an equal chance for selecting a current agent and a random agent if the global best is not used.

\section{Experimental Designs for Hyperparameter Settings}\label{sec:DE.designs}

Different design strategies address diverse experimental objectives and constraints. For instance, factorial and fractional factorial designs are widely used to study the main effects and interactions of factors systematically, while response surface designs, such as central composite designs, support optimization and curvature estimation \parencite{myers2016response}. In other cases, space-filling designs such as Latin hypercube designs, maximin distance designs, and maximum projection designs are preferred for high-dimensional and computational experiments \parencite{joseph2016space}. Each {design} has its {own} unique strengths and drawbacks. The choice would depend on the experimental goals, computational resources, and the nature of the underlying system being studied. Through thoughtful design construction, researchers can ensure that their experiments are not only scientifically rigorous but also cost-effective and impactful.

Various designs can be used to set the DE hyperparameters before the optimization process. As the functions optimized by DE are often complex with many local minima, one has to carefully choose the initial points for the optimization process. These initial points can be determined using any of the methods discussed below.


%Data was generated using various designs. These designs include the Full factorial design (FFD), Latin hypercube design (LHD), Uniform Projection design (UPD), maximin designs, maximum projection design (MaxPro), Central composite design (CCD) and  Orthogonal Array Composite Design (OACD).

\subsection{Factorial designs}
Factorial designs are a research method for studying the effects of multiple independent variables on a response variable, formalized by R.~A.~Fisher in the early 20th century \parencite{fisher1935}. Full factorial designs {consist} of a grid of all possible level  combinations to achieve uniform coverage across the design space.  These designs typically involve factors at two or three levels, resulting in \(2^m\) or \(3^m\) observations for \(m\) factors.
% Figure \ref{fig:ffd} shows a $2^3$ full factorial design.
%Full factorial designs sample points at the corners of a hypercube, ensuring uniform distribution across the design space.
They allow for the analysis of main effects and interactions \parencite{montgomery2017design}, but can require larger sample sizes for adequate power \parencite{tabachnick2019}.

\begin{description}
  \item Fractional factorial designs were introduced by  \textcite{finney1945fractional} to mitigate the need for larger samples. These designs use a fraction of runs based on the effect hierarchy ordering principle \parencite{wu2011experiments}, focusing on main effects and lower-order interactions. They are expressed as \(2^{m-p}\) or \(3^{m-p}\), and defined by setting some factors as products of others. Selecting defining relations for fractional designs is essential, with criteria such as maximum resolution and minimum aberration guiding this process \parencite{wu2011experiments}.

\item
Central composite designs (CCDs) were introduced by \textcite{box1951series} as an extension of factorial designs. They were developed as a way to efficiently fit quadratic response surfaces and identify optimal process settings in industrial experiments. CCDs are full or fractional factorial designs that are augmented with two additional sets of sampling points described as ``center'' and ``axial or star'' points \parencite{box1951series}. The center point is defined by all factors being set at their center level. The CCD uses $2m$ axial points, each of which is defined by all but one factor being at their center level and the level of the remaining factor is denoted by $\alpha$, which is generally chosen to be between 1 and $\sqrt{m}$ \parencite{montgomery2017design}.


%\subsection{Orthogonal arrays (OA)} \reva{(This paragraph can be deleted?)}
%An orthogonal array is a structured arrangement of experimental runs that ensures each factor level combination occurs with equal frequency. In other words, orthogonal arrays provide a systematic way to design experiments in a balanced manner, allowing for efficient and effective exploration of multiple factors and their interactions \parencite{box1978statistics, wu2011experiments}. While the full, fractional, and central composite designs are described as ``regular'' designs because the effect of each factor is either estimated independently of all others or fully aliased with one of the other factors, orthogonal arrays are described to be ``non-regular'' as they do not possess this property \parencite{burton2022design}. In
%contrast to regular fractional factorial designs where some factorial effects are fully aliased, orthogonal array-based designs can incorporate partial aliasing which could capture the effect of all two-factor interactions using a small number of runs.
%More specifically, an orthogonal array-based design
%$O A\left(n, s_1^{m_1} s_2^{m_2} \ldots s_\gamma^{m_\gamma}, t\right)$ is one that has an $n \times m$ design matrix where $n$ is the number of runs and $m=m_1+\cdots+m_\gamma$.
%The strength of the orthogonal array $O A$ is denoted by $t$, whereby $m_i$ columns have $s_i \geq 2$ levels such that, within any $t$ columns, all possible level combinations occur equally often. Note that $\gamma=1$ corresponds to the case where all factors have the same number of levels and the associated designs are described as symmetrical orthogonal arrays. Designs that have $\gamma > 1$ are described asymmetric or mixed-level orthogonal arrays \parencite{burton2022design}.
%One advantage of OAs over regular fractional factorial designs is that the $OA$ designs are more efficient and can accommodate mixed level factors with a small number of runs \parencite{ding2013use, jaynes2016use, xu2014combining}


\item Orthogonal array composite designs (OACDs), introduced by \textcite{xu2014combining}, are a class of composite designs based on a two-level factorial design and a three-level {orthogonal array (OA)}. An $\mathrm{OA}$ of $n$ runs, $m$ columns, $s$ levels, and strength $t$, denoted by $\mathrm{OA}\left(n, s^m, t\right)$, is an $n \times m$ matrix in which all $s^t$ level-combinations appear equally often in every $n \times t$ submatrix \parencite{wu2011experiments}. For example, a $2^m$ factorial design can be viewed as $\operatorname{OA}\left(n, 2^m, t\right)$ with $n=2^m$ and $t=m$. Similarly, a three-level OA can be written as $\operatorname{OA}\left(n, 3^m, t\right)$. Thus, an OACD is a composite design which consists of a two-level factorial design as its factorial points, a three-level OA as its additional points, plus any number of center points \parencite{luna2022orthogonal}.
\end{description}

\subsection{Space-filling designs}
While the previously discussed designs utilize sampling points that are at the boundaries of the design space, space-filling designs generate samples that are dispersed throughout the multidimensional design space and not just at the boundary of the design space. These designs are important in sampling a surface as they could capture important regions thereby minimizing the bias between the true structure of the surface and the estimated surface from the sampled points \parencite{gardner2006small, giunta2003overview}. {They} are of various types depending on the approach used to construct them, e.g., sampling-based -- Latin Hypercube designs, and distance-based -- maximin distance designs. %, and distribution-based --  uniform designs \parencite{burton2022design}.

\begin{description}
  \item Latin hypercube designs (LHDs) -- Based on \textcite{mckay1992latin}'s Latin hypercube sampling, it divides the range of each factor into  bins of equal size, where $n$ also corresponds to the number of samples to be generated resulting in a total of $n^m$ combinations where $m$ is the number of factors  being considered. The $n$ samples are then randomly generated such that for all one-dimensional projections, there will be only one sample in each bin.
  \item Maximin distance designs -- Introduced by \textcite{johnson1990minimax}, this design aims at spreading out the design points in the design space by maximizing the minimum  distance between any two design points. It thus tends to place a large proportion of points at the corners and on the boundaries of the design space. Mathematically, this can be formulated as follows. Suppose we want to construct an $n$-run design in $m$ factors. Let the design region be $\mathcal{X}$ and let the design be $D=\left\{\x_1 \ldots, \x_n\right\}$, where each design point {$\x_{i}$} is in $\mathcal{X}$. The maximin distance design optimizes the function below:
$$
\max _D \min _{i \ne j} d(\x_i, \x_j),
$$
where $d(\x_i, \x_j)=\left\{ {\sum_{k=1}^m (x_{ik}-x_{jk})^2} \right\}^{1/2}$ is the Euclidean distance between the points $\x_i$ and $\x_j$.
  \item Maximin LHDs -- Unlike LHDs,  maximin distance designs do not have good projection properties for each factor. \textcite{morris1995exploratory} proposed to overcome this problem by searching for the maximin distance design within the class of LHDs. They also proposed to use the following criterion to achieve maximin distance:
\begin{equation}
\min _D\left\{\sum_{i=1}^{n-1} \sum_{j=i+1}^n \frac{1}{d^p(\x_i, \x_j)}\right\}^{1 / p}
\label{morris_mitchelle}
\end{equation}
where $p > 0$ is chosen large enough, say $p=15$.

  \item Maximum projection (MaxPro) designs  -- Although maximin LHDs ensure good space-filling in $m$ dimensions and uniform projections in each dimension, their projection properties in two to $m - 1$ dimensions are not known. By the effect sparsity principle \parencite{wu2011experiments}, only a few factors are expected to be important. To curb this, \textcite{joseph2015maximum} proposed a different criterion:
\begin{equation}
\min _D \psi(D)=\left\{\frac{2}{n(n-1)} \sum_{i=1}^{n-1} \sum_{j=i+1}^n \frac{1}{\prod_{k=1}^m\left(x_{ik}-x_{jk}\right)^2}\right\}^{1 / m} .
\end{equation}
They showed that the design minimizing $\psi(D)$ tends to maximize its projection capability in all subspaces of factors, and thus named these designs as maximum projection designs.
\end{description}


\section{Surrogate Models}\label{sec:models}
We consider three  types of surrogate models to describe the response surface of the DE hyperparameters, namely, linear model, kriging model and heterogeneous Gaussian Process (HetGP). We describe each briefly in the following.

\subsection{{Linear Model (lm)}}
Linear models are widely used to model the relationship between the response variable and the inputs (i.e., factors). Here we adopt a second-order model which is commonly used to model data from physcical experiments.
For $m$ quantitative factors, denoted by $x_1, \ldots, x_m$, a second-order model is defined as
\begin{equation}
y=\beta_0+\sum_{i=1}^m \beta_i x_i+\sum_{i=1}^m \beta_{i i} x_i^2+\sum_{i<j} \beta_{i j} x_i x_j+\epsilon
\end{equation}
where $\beta_0, \beta_i, \beta_{i i}$, and $\beta_{i j}$ are the intercept, linear, quadratic, and bilinear (or interaction) terms, respectively, and $\epsilon$ is the error term. This model is simple and provides a straightforward way to model and understand relationships between the response variable and the factors. % The main effects are easy to  interpret.  % In R the $lm$ function was used to fit this model.

\subsection{Kriging Model (km)}
Proposed by South African geostatistician \textcite{krige1951statistical}, kriging is one of the methods used to interpolate intermediate values, whereby these intermediate values are modeled using Gaussian Process (GP) which is governed by prior co-variances. It provides a probabilistic prediction of the output variable, as well as an estimate of the uncertainty of the prediction \parencite{chevalier2014kriginv}. The kriging predictors interpolating the observations are assumed to be noise-free \parencite{roustant2012dicekriging}. Intermediate interpolated values obtained by kriging are the best linear unbiased predictors. % There are two forms of fitting a kriging model:  Simple Kriging and Universal Kriging.

%\reva{(not helpful to talk about simple kriging.)}
%\iffalse %
%In simple kriging (SK), $Y(\x)$ is assumed to be the sum of a known deterministic trend function and a centered square-integrable process:
% \begin{equation}
%       Y(\x) = \mu(\x) + Z(\x),
% \end{equation}
% where \(\mu(\x)\) is the trend function and \(Z(\x)\) is a stationary GP with zero mean and covariance function \(\psi\). % that is known %and \(\epsilon \sim N(0,\tau^2)\) is a random error term independent of \(Z(x)\).
% In Universal Kriging (UK), the trend $\mu(\x)$ is known up to a set of linear trend and unknown coefficients. This linear trend is set to be the linear combination of some fixed basis functions $f_i$'s.
% That is
% \begin{equation}
% \mu(\x)=\sum_{i=1}^k \beta_i f_i(\x) .
% \end{equation}\\
% UK consists of deriving the best linear predictions of $Y(\x)$ based on the observations
% while estimating the vector $\beta:=(\beta_1, \dots,\beta_k)^\top$ on the fly. Note that in the specific case where the basis functions reduce to a unique constant function, UK is referred to as ordinary Kriging (OK) \parencite{roustant2012dicekriging}.
% \fi
Kriging models are widely used to model deterministic functions in computer experiments. The kriging model consists of two parts: a trend and a GP. The trend part is often modeled as a regression on some fixed basis functions. In the specific case where the basis functions reduce to a constant function, it is referred to as ordinary kriging  \parencite{roustant2012dicekriging}. The general form is as given below
\begin{equation}\label{eq:UK}
      Y(\x) = \sum_{i=1}^k \beta_i f_i(\x) + Z(\x),
\end{equation}
where $f_1, \ldots, f_k$ are $k$ basis functions, $\beta_1, \dots,\beta_k$ are corresponding regression coefficients, and  \(Z(\x)\) is a stationary GP with zero mean and covariance function \(\psi\). % that is known %and \(\epsilon \sim N(0
The covariance function $\psi$ completely defines the behavior of the GP $Z(\x)$. It is defined as
\begin{equation}
%\resizebox{0.5\textwidth}{!}{$
\psi\left(\x_{i},\x_{j}\right) =\operatorname{Cov} \left(Z\left(\x_{i}\right), Z\left(\x_{j}\right)\right) = \sigma^{2} \prod_{l=1}^{m} K\left(h_{l} ; \theta_{l}\right),
%$}
\end{equation}
where $\sigma^2$ is the scale parameter called the process variance,
$h_l = |x_{i,l}-x_{j,l}|$, $x_{i,l}$ and $x_{j,l}$ are the $l$th elements of the $i^{th}$ run $\x_i$ and the $j^{th}$ run $\x_j$, and $K(h ; \theta)$ is the correlation function.
%For a stationary GP, the mean and covariance remain constant with respect to time.
The parameters $\theta_l$ chosen for the correlation function $K\left(h_{l};\theta_{l}\right)$  must be positive. Otherwise the correlation function will not be feasible. These parameters are chosen to be physically interpretable in the same unit as the corresponding variables. They are often referred to as the \textit{characteristic length-scales} by \textcite{rasmussen2006gaussian}. Popular correlation functions include Gaussian, Mat\'ern, and power-exponential family correlation functions. The Mat\'ern function with parameter $\nu = 5/2$ is often chosen as the default when fitting kriging models. It is defined as:
\begin{equation}
K(h ; \theta)=\left(1+\frac{\sqrt{5} h}{\theta}+\frac{5 h^{2}}{3 \theta^{2}}\right) \exp \left(-\frac{\sqrt{5} h}{\theta}\right).
\end{equation}
The unknown parameters can be estimated via MLE or cross validation. We use the  \texttt{km} function  in the {DiceKriging package in R} \parencite{roustant2012dicekriging} to fit the kriging model.



\subsection{{Heteroskedastic Gaussian Process (HetGP)}}

HetGP is useful for modeling simulation experiments exhibiting input-dependent noise. It employs a mean zero GP and shifts all of the modeling effort to the covariance structure \parencite{binois2018practical}. Specifically, the HetGP model is given by
\begin{equation}
y_i = y\left(\x_i\right)=f\left(\x_i\right)+\varepsilon_i, \quad\varepsilon_i \sim \mathcal{N}\left(0, r\left(\x_i\right)\right),
\end{equation}
where $f(\x_i)$ is a GP with covariance or kernel $k(\cdot, \cdot)$ and $r(\x_i)$ is the variance of $\epsilon_i$ which depends on $\x_i$. The  kernel $k(\cdot, \cdot)$ is positive definite, with parameterized families such as the Gaussian or Mat\'ern being typical choices. If $r(\x_i)=\tau^2$ is a constant, then the process is homoskedastic.
In matrix notation, the modeling framework just described is equivalent to writing
$$
Y(\x) \sim \mathcal{N}\left(\mathbf{0}, \mathbf{K}_n+\boldsymbol{\Sigma}_n\right),
$$
where $\mathbf{K}_n$ is the $n \times n$ matrix with $(i, j)$ coordinate $k(\x_i, \x_j)$, and $\boldsymbol{\Sigma}_n=\operatorname{Diag}\left(r\left(\x_1\right), \ldots, r\left(\x_n\right)\right)$ is the variance matrix of the  vector of independent noise $\varepsilon_i$.

Given the kernel function $k(\cdot, \cdot)$ and data $\y=(y_1,\ldots,y_n)^{\top}$, multivariate normal conditional identities provide a predictive distribution at site $\x: Y(\x) \mid \y$, which is Gaussian with parameters
$$
\begin{aligned}
\mu(\x) & =\mathbb{E}(Y(\x) \mid \y)=\mathbf{k}(\x)^{\top}\left(\mathbf{K}_n+\mathbf{\Sigma}_n\right)^{-1} \y,  \\
\sigma^2(\x) & =\mathbb{V} \operatorname{ar}(Y(\x) \mid \y)=k(\x, \x)+r(\x)-\mathbf{k}(\x)^{\top}\left(\mathbf{K}_n+\mathbf{\Sigma}_n\right)^{-1} \mathbf{k}(\x) ,
\end{aligned}
$$
where $\mathbf{k}(\x)=\left(k\left(\x, \x_1\right), \ldots, k\left(\x, \x_n\right)\right)^{\top}$.
We use the \texttt{mleHetGP} function in the {hetGP package in R} \parencite{binois2021hetgp} to fit this model with the default Gaussian kernel.


\section{The Data Generation Process}
%\reva{(I've made many changes in this section. Revise as needed.)}

We aim to obtain optimal DE hyperparameter settings that can be used to generate UPDs. %The objective function is the uniform projection criterion. %This is then minimized and the minimum returned as the response value.

\subsection{Objective Function: Uniform Projection Criterion}
Proposed by \textcite{sun2019uniform}, the uniform projection criterion solely focuses on  two-dimensional projections. This is due to two factor interactions being more important than three-factor or higher-order interactions. The motivating idea was that although designs with low discrepancy have good uniformity in the full-dimensional space, they can have bad projections in lower dimensional spaces, which is undesirable when only a few factors are active. Thus designs with better projection properties are preferred. \textcite{sun2019uniform} argued that  UPDs scatter points uniformly in all dimensions and have good space-filling properties in terms of distance, uniformity and orthogonality.

The uniform projection criterion is defined using the centered $L_2$-discrepancy.
For an $n \times m$ design $D=\left(x_{i k}\right)$ with $s$ levels from $\{0, 1, \ldots, s-1 \}$, its (squared) centered $L_2$-discrepancy is defined as
$$
\begin{aligned}
  \mathrm{CD}(D)= & \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n
  \prod_{k=1}^m\left(1+\frac{1}{2}\left|z_{i k}\right| +
  \frac{1}{2}\left|z_{j k}\right| -
  \frac{1}{2}\left|z_{i k}-z_{j k}\right|\right) \\
  & -\frac{2}{n} \sum_{i=1}^n \prod_{k=1}^m\left(1+\frac{1}{2}
  \left|z_{i k}\right|-\frac{1}{2}\left|z_{i k}\right|^2\right)+
  \left(\frac{13}{12}\right)^m,
  \end{aligned}
$$
where $z_{i k}=\left(2 x_{i k}-s+1\right) /(2 s)$. Then the uniform projection criterion is to minimize
\begin{equation}
  \phi(D)=\frac{2}{m(m-1)} \sum_{|u|=2} \mathrm{CD}\left(D_u\right),
  \label{upd}
\end{equation}
where $u$ is a subset of $\{1,2, \ldots, m\},|u|$ denotes the cardinality of $u$ and $D_u$ is the projected design of $D$ onto dimensions indexed by the elements of $u$. The $\phi(D)$ value is the average centered $L_2$-discrepancy values of all two-dimensional projections of $D$.

The uniform projection criterion defined in \eqref{upd} requires $O(n^2 m^2)$ operations. \textcite{sun2019uniform} derived an alternative formula for computing  the $\phi(D)$ value which requires only $O(n^2 m)$ operations.  They showed that $\phi(D)$  is a function of  the pairwise $L_1$-distances between design points for balanced designs (i.e., each level appears equally often in any factor).
Specifically, for a balanced design $D=\{\x_1, \ldots, \x_n \}$ with $n$ runs,  $m$ factors, and $s$ levels, they showed
\begin{equation}\label{eq:distance}
\phi(D) =  {\frac{g(D)}{4m(m-1)n^2s^2} }+C(m, s),
\end{equation}
	where
\begin{equation}\label{eq:g(D)}
g(D)=\sum^n\limits_{i=1}\sum^n\limits_{j=1}d^2_1(\x_i, \x_j)- \frac{2}{n}\sum^n\limits_{i=1}\bigg(\sum^n\limits_{j=1}d_1(\x_i, \x_j)\bigg)^2,
\end{equation}
$d_1(\x_i, \x_j)=\sum_{k=1}^m |x_{ik}-x_{jk}|$ is  the $L_1$-distance between $\x_i$ and $\x_j$, and
$$C(m, s)=	\frac{4(5m-2)s^4+30(3m-5)s^2+15m+33}{720(m-1)s^4} + \frac{1+(-1)^s}{64 s^4}.$$
We compute the $\phi(D)$ value based on \eqref{eq:distance} and \eqref{eq:g(D)} for its computational efficiency. Because the $\phi(D)$ values are typically small, to ease the  presentation, we report the $1000 \times \phi(D)$ values throughout the paper.

We implement the DE algorithm and the uniform projection criterion in the package \texttt{UniPro}. % \texttt{Meta4Design}.
The following code generates an $n\times m$ UPD with $s$ levels
\begin{Schunk}
\begin{Sinput}
> UniPro(n, m, s, NP, itermax, pMut, pCR, pGBest, seed)
\end{Sinput}
\end{Schunk}
where $NP$, $itermax$, $pMut$, $pCR$ and $pGBest$ are DE hyperparameters described in Section 2, and $seed$ is an optional seed for random number generators that ensures reproducibility.

As the task of design generation is quite complex, we consider 3 design sizes.  A UPD of size $30\times3$ is considered as a small and easy task, $50\times5$ as a medium task and $70\times 7$ as a large and difficult task. We only consider the construction of designs with $s=n$ so that the resulting UPD is an LHD.

\subsection{Training and testing data}

Designs discussed in Section \ref{sec:DE.designs} are used to determine the parameter settings for the DE algorithm hyperparameters. Specifically, we construct five designs: a CCD with 43 runs (\verb|ccd3_43|), an OACD with 50 runs (\verb|oacd3_50|), a 50-run random LHD (\verb|lhd_50|), a 50-run maximin LHD (\verb|maximin_50|), and a 50-run maxpro LHD (\verb|maxpro_50|). The CCD consists of a $2^5$ full factorial, 10 axial points and 1 center point. The OACD consists of a $2^5$ full factorial and an OA$(18, 3^5, 2)$. The maximin and maxpro LHDs are generated using R packages SLHD and MaxPro, respectively. All designs consider five factors, each corresponding to one hyperparameter. Each run represents a unique combination of the hyperparameter values. The CCD and OACD have 3 levels while the rest have 50 levels. The levels are linearly interpolated within the minimum and maximum factor values for each hyperparameter.

Given the target design size $(n \times m)$ and a setting of the five hyperparameters, we run the \texttt{UniPro} function to generate an $n \times m$ UPD and the resulting $\phi(D)$ value defined in  \eqref{upd}. This is recorded as the response value for that particular hyperparameter setting and target design size. For each setting, the DE algorithm is replicated ten times yielding ten replicates for the response. These are then aggregated to obtain the mean and the standard deviation of the response.
Thus we obtain five training datasets for each target size.


The same procedure is taken to generate the testing datasets with the exception that the designs used for the hyperparameter setting combinations being a $3^5$ full factorial design (\verb|full_243|), a random LHD with 243 runs (\verb|lhd_243|), a combination of these two (\verb|full_243+lhd_243|), and a $4^5$ full factorial design (\verb|full_1024|). %\reva{Testing on these data seems good enough. This is because the random LHD enjoys the maximum space-filling property in all one dimensions, while the $3^5$ and $4^5$ factorial designs cover the entire 5-dimensional input space in a uniform fashion.} % \parencite{shi2023evaluating}.
{Testing with these datasets is deemed sufficient because the random LHD exhibits maximum space-filling properties in each dimension, while the $3^5$ and $4^5$ factorial designs ensure uniform coverage of the entire 5-dimensional input space.}

Density plots of the response for the testing and training data are presented in Figure \ref{fig:density} for the target size $50\times 5$.  All of the distributions are skewed to the right. For a $50\times5$ UPD all the training designs lead to similar minimum $\phi(.)$ values, around 0.17, whereas different types of designs lead to different maximum $\phi(.)$ values. Indeed, all space-filling designs have  maximum $\phi(.)$ values around 0.28, while the factorial designs and the combined design have a maximum $\phi(.)$ values around 0.34. The narrower range of the $\phi(.)$ values suggests that the space-filling designs do not explore the entire space of hyperparameters.
%The space-filling designs have a smaller range of the $\phi(.)$ values than the factorial designs. The maximum

\begin{figure}%[!ht]
    \centering

\includegraphics{pdfs/density}
  \caption{Density plots of the $\phi(D)$ values with target size $50\times5$}
    \label{fig:density}
\end{figure}


\subsection{{Model evaluation}}
For each training dataset, we fit the three models descibed in Section \ref{sec:models} and test on the four testing datasets. Designs are evaluated by considering their ability to collect informative data for building a statistical model that specifies the relationship between the response and the hyperparameters, which is measured by the test root mean squared error (RMSE). For a test datset with $N$ runs, the RMSE is calculated as
\[ RMSE = \left\{ \frac{1}{N} \sum_{i=1}^N (y_i - \hat y_i)^2 \right\} ^{1/2} , \]
where $y_i$ and $\hat y_i$ are the $i$th observed and predicted response, respectively.
The correlation between the observed and  predicted responses is also reported.
A small RMSE value indicates a good fit while a large correlation is preferable.

%\clearpage
\section{Results and Analysis}

%\reva{(Merge Table \ref{tab:tab1} and Table \ref{tab:tab2} into one table)}
%There are various striking observations made from the analysis done. Tables 1-2 are results for the class considered as small. That is, the target size of the generated Uniform projection design is $30\times 3$.


\begin{table}

\caption{\label{tab:tab1}Comparison of designs and model evaluations with target size $30 \times 3$}
\centering
\begin{tabular}[t]{|l|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\textbf{ }} & \multicolumn{6}{|c|}{\textbf{(a) Testing on the $3^5$ FFD}} & \multicolumn{6}{|c|}{\textbf{(b) Testing on the 243 LHD}} \\
\cline{2-7} \cline{8-13}
\multicolumn{1}{|c|}{ } & \multicolumn{3}{|c|}{correlation} & \multicolumn{3}{|c|}{RMSE} & \multicolumn{3}{|c|}{correlation} & \multicolumn{3}{|c|}{RMSE} \\
\cline{2-4} \cline{5-7} \cline{8-10} \cline{11-13}
Design & lm & km & hetGP & lm & km & hetGP & lm & km & hetGP & lm & km & hetGP\\
\hline
ccd3\_43 & 0.88 & 0.88 & 0.88 & 1.51 & 1.59 & 1.62 & 0.65 & 0.03 & 0.60 & 1.62 & 3.75 & 1.51\\
\hline
oacd3\_50 & 0.88 & 0.94 & 0.93 & 1.62 & 1.25 & 1.32 & 0.68 & 0.63 & 0.61 & 1.94 & 2.30 & 2.22\\
\hline
lhd\_50 & 0.41 & 0.36 & 0.34 & 3.19 & 3.30 & 3.27 & 0.28 & 0.20 & 0.19 & 1.08 & 1.14 & 1.14\\
\hline
maximin\_50 & 0.71 & 0.73 & 0.74 & 2.86 & 3.19 & 3.03 & 0.64 & 0.63 & 0.64 & 0.66 & 0.66 & 0.65\\
\hline
maxpro\_50 & 0.71 & 0.62 & 0.66 & 2.35 & 2.97 & 2.72 & 0.69 & 0.71 & 0.74 & 0.74 & 0.69 & 0.61\\
\hline


\multicolumn{1}{|c|}{\textbf{ }} & \multicolumn{6}{|c|}{\textbf{(c) Testing on the $3^5$ FFD+243 LHD}} & \multicolumn{6}{|c|}{\textbf{(d) Testing on the $4^5$ FFD}} \\
\cline{2-7} \cline{8-13}
\multicolumn{1}{|c|}{ } & \multicolumn{3}{|c|}{correlation} & \multicolumn{3}{|c|}{RMSE} & \multicolumn{3}{|c|}{correlation} & \multicolumn{3}{|c|}{RMSE} \\
\cline{2-4} \cline{5-7} \cline{8-10} \cline{11-13}
Design & lm & km & hetGP & lm & km & hetGP & lm & km & hetGP & lm & km & hetGP\\
\hline
ccd3\_43 & 0.84 & 0.60 & 0.83 & 1.57 & 2.88 & 1.57 & 0.84 & 0.66 & 0.85 & 1.55 & 2.83 & 1.50\\
\hline
oacd3\_50 & 0.82 & 0.83 & 0.83 & 1.79 & 1.85 & 1.83 & 0.85 & 0.87 & 0.87 & 1.68 & 1.68 & 1.68\\
\hline
lhd\_50 & 0.42 & 0.35 & 0.36 & 2.38 & 2.47 & 2.45 & 0.45 & 0.39 & 0.37 & 2.47 & 2.57 & 2.56\\
\hline
maximin\_50 & 0.69 & 0.63 & 0.68 & 2.08 & 2.31 & 2.19 & 0.74 & 0.75 & 0.76 & 2.16 & 2.38 & 2.27\\
\hline
maxpro\_50 & 0.74 & 0.60 & 0.68 & 1.75 & 2.15 & 1.98 & 0.74 & 0.67 & 0.71 & 1.80 & 2.22 & 2.03\\
\hline
\end{tabular}
\end{table}

\begin{figure}%[!ht]
\centering
\includegraphics[height=5in, width=6in]{pdfs/barplots1}
\caption{Comparison of RMSE with target size $30\times3$}
\label{barplots1}
\end{figure}

%\reva{Table \ref{tab:tab1} and Figure \ref{barplots1} present comparison of designs and model evaluations in terms of correlation and RMSEs with target size $30\times 3$. We first examine the results for testing the two 243-run data sets in Table \ref{tab:tab1}(a)(b) and Figure \ref{barplots1}(a)(b).}

{Table \ref{tab:tab1} and Figure \ref{barplots1} compare designs and model evaluations based on correlation and RMSEs for a target size of \(30\times 3\). We begin by analyzing the results for testing the two 243-run datasets, as shown in Table \ref{tab:tab1}(a)(b) and Figure \ref{barplots1}(a)(b).}
%Table \ref{tab:tab1}(a)(b) and Figure \ref{barplots1}(a)(b) present comparison of designs and model evaluations with target size $30\times 3$ for testing the two 243-run data sets.
%There are various striking observations made from the analysis done. Tables 1-2 are results for the class considered as small. That is, the target size of the generated Uniform projection design is $30\times 3$.
One striking observation is that the performance of the training data set depends on the nature of the testing data set. The  composite designs, CCD and OACD, seem to be better when tested on the $3^5$ full factorial design (FFD) while the space-filling designs (random LHD, maximin LHD, and maxpro LHD) did better when tested on the 243-run random LHD.
As this does not give a general idea as to which designs might perform better in general, we invoke the 486-run combined design ($3^5$ FFD + 243-run LHD) and the $4^5$ FFD as the testing dataset.
Here we see that the composite designs perform better than the space-filling designs; see Table \ref{tab:tab1}(c)(d) and Figure \ref{barplots1}(c)(d). The 50-run random LHD performed the worst in terms of correlation regardless of the testing data. The correlation is strikingly low whereas the RMSE is high. This might be due to randomness, but it does show the weakness of the random LHD. %, but the result is not replicated in the $50\times 5$ and $70\times 7$ design sizes.

One bizarre observation from Table \ref{tab:tab1}(b) is the correlation of $0.03$ when using the CCD as the training design and testing it on the 243-run random LHD. This value is strikingly lower than any other values given in Table \ref{tab:tab1}. No apparent reason could be deduced as to why this is so. Multiple replications indicated that this is not an error.  From all the results, we can deduce the robustness of OACD over CCD. This gives a reason to use OACD for the hyperparameter initialization.




%When using the training as the testing dataset, the kriging model gives a correlation of 1 and RMSE of 0. This is expected since the kriging perfectly interpolate the known values at the training locations while assuming a stationary covariance.  This assumption cannot be proven as the covariance between two points in the DE hyperparameter surface structure depends cannot be shown to depend only on the distance or spatial lag between those points, and not on their specific locations within the domain. Due to this, the heteroskedastic gaussian process is preferred.


%In addition, the performance of the CCD and OACD training datasets when tested on the $3^5$ factorial design was better than when tested on the $4^5$ factorial design.  This could be attributed to the fact that there are $4$ levels in the testing dataset yet only $3$ in the training dataset, thus there are ``holes" in the experimental space because the testing phase introduces two levels that have not been explored during training. These gaps may lead to uncertainties in predicting the response at the unexplored level and thereby get poor results as compared to the $3^5$ factorial.


With regards to the models, there seems to be no striking observation to be made as to whether one fitting method performs better than the other two, with  exception for one $30 \times 3$ case when the kriging model fitting to the CCD training data had a much higher RMSE value than the other cases.
%Although using the $3^5$ full factorial design as the training dataset and then testing on the combined dataset, it can be noted that the Heteroskedastic Gaussian model is significantly better than both kriging and linear model. As HetGP enjoys more data points, this might be the reason of its better performance. Thus reasonable to conclude that Heterogeneous Gaussian model should be preferred even when the linear model does result in almost similar results.


The three models have quite different assumptions. The linear model assumes a polynomial trend and independent random errors with homoskedastic variance. The kriging model assumes a stationary covariance structure, that is, the covariance between two points in the DE hyperparameter surface structure depends only on the distance or spatial lag between those points, and not on their specific locations within the domain. The HetGP model assumes a heteroskedastic variance-covariance structure. For the DE algorithm, the homoskedastic and stationary assumptions are questionable. Due to this, the HetGP model is preferred to the linear model and the kriging model. However, the linear model is easy to interpret and fits as well as the HetGP model, and from the results, there is no striking difference between the two. Therefore, we use the linear model to determine factor importance and optimal hyperparameter settings.

%. On the other hand, space-filling designs do not capture well the vertex/boundary information.


Tables \ref{tab:tab2}-\ref{tab:tab3} and
Figures \ref{barplots2}-\ref{barplots3} in the Appendix show results when the target design sizes are $50\times 5$ and $70\times7$, respectively.
Looking at the results, apart from the random LHD with 50 runs, previously stated observations are upheld. % The composite designs perform better when tested on the $3^5$ FFD while the space-filling designs perform better when tested on the 243-run LHD. When tested on the  486-run combined data and the $4^5$ FFD, the composite designs perform better than the space-filling designs, although the difference tends to be small as the values are very close. In addition, there is no clear difference between the performances of the three models.
% indicating that perhaps in large designs, there won't be a distinction between the space-filling designs and the composite designs. %This is probably because as the dimension increases, most of the space within the cube is empty, since everything is almost pushed to the boundary, hence both the space-filling designs and the factorial composite designs would be expected to yield almost similar prediction results.


A natural question is why composite designs perform better than the space-filling designs. We perceive that the hyperparameters at the boundaries lead to some extreme cases in this experiment and the composite  designs do capture this phenomena while the space-filling designs do not. Figure \ref{fig:distance} presents the histograms of the distances from design points to the design center for all the designs, where each column is rescaled to $[-1, 1]$ and  the euclidean distance from each point to the center of the design is calculated. This gives an insight as to why the composite designs, OACD and CCD, tend to perform better than the space-filling designs. This is because the composite designs tend to capture information lying at the boundaries compared to the space-filling designs which tend to capture the information lying at the center of the design. This is confirmed by the notion that three of the hyperparameters tend to be optimized around their highest level as discussed in the next section.



\begin{figure}%[!ht]
    \centering
    %\includegraphics[scale = 0.5]{images/distance.png}
    \includegraphics{pdfs/histogram}
    \caption{Histogram of the distances from  design points to the design center}
    \label{fig:distance}
\end{figure}


%\clearpage
\section{Factor Importance and Optimal Settings}
%\reva{(I've made many changes in this section. Revise as needed.)}
The results obtained call for a deeper look into the model and how each factor is involved in the surface approximation. This enables us to have a better picture of the surface generated by the DE hyperparameters.

\iffalse % R code to generate the table
\begin{table}
\begin{center}
\begin{tabular}{l c c c c}
\hline
 & ccd3\_43 & oacd3\_50 & maximin\_50 & maxpro\_50 \\
\hline
(Intercept)    & $0.3815^{***}$  & $0.3876^{***}$  & $0.3878^{***}$  & $0.3814^{***}$  \\
NP             & $-0.0134^{***}$ & $-0.0143^{***}$ & $-0.0042^{***}$ & $-0.0055^{*}$   \\
pMut           & $0.0020$        & $0.0028$        & $0.0043^{***}$  & $0.0045^{*}$    \\
pGBest         & $-0.0204^{***}$ & $-0.0224^{***}$ & $-0.0047^{***}$ & $-0.0082^{***}$ \\
pCR            & $-0.0022$       & $-0.0030$       & $0.0004$        & $0.0007$        \\
itermax        & $-0.0146^{***}$ & $-0.0152^{***}$ & $-0.0046^{***}$ & $-0.0021$       \\
itermax\_q     & $0.0090$        & $-0.0081$       & $0.0011$        & $0.0057$        \\
NP\_q          & $0.0119$        & $0.0080$        & $0.0022$        & $-0.0004$       \\
pCR\_q         & $0.0071$        & $0.0074$        & $-0.0008$       & $0.0049$        \\
pGBest\_q      & $0.0083$        & $0.0192^{*}$    & $0.0035$        & $0.0159^{***}$  \\
pMut\_q        & $0.0150$        & $0.0174^{*}$    & $0.0083^{***}$  & $0.0104^{*}$    \\
NP:pMut        & $0.0058$        & $0.0064^{*}$    & $-0.0002$       & $-0.0037$       \\
NP:pGBest      & $-0.0041$       & $-0.0038$       & $0.0017$        & $0.0006$        \\
NP:pCR         & $0.0002$        & $-0.0007$       & $-0.0006$       & $-0.0002$       \\
NP:itermax     & $0.0049$        & $0.0033$        & $0.0023$        & $0.0052$        \\
pMut:pGBest    & $-0.0080^{*}$   & $-0.0093^{***}$ & $-0.0089^{***}$ & $-0.0139^{**}$  \\
pMut:pCR       & $0.0203^{***}$  & $0.0197^{***}$  & $0.0042^{**}$   & $0.0018$        \\
pMut:itermax   & $0.0032$        & $0.0025$        & $-0.0061^{***}$ & $0.0004$        \\
pGBest:pCR     & $0.0034$        & $0.0036$        & $0.0000$        & $0.0051$        \\
pGBest:itermax & $0.0057$        & $0.0068^{**}$   & $0.0076^{***}$  & $0.0058$        \\
pCR:itermax    & $0.0037$        & $0.0021$        & $0.0018$        & $0.0040$        \\
\hline
R$^2$          & $0.9034$        & $0.9235$        & $0.8970$        & $0.7297$        \\
Adj. R$^2$     & $0.8157$        & $0.8708$        & $0.8260$        & $0.5433$        \\
Num. obs.      & $43$            & $50$            & $50$            & $50$            \\
\hline
\multicolumn{5}{l}{\scriptsize{$^{***}p<0.001$; $^{**}p<0.01$; $^{*}p<0.05$}}
\end{tabular}
\caption{Statistical models}
\label{table:coefficients}
\end{center}
\end{table}\fi

%% Modify the table by replacing \_q with $^2$ and the caption
\begin{table}
\begin{center}
\begin{tabular}{l c c c c}
\hline
% & ccd3\_43 & oacd3\_50 & maximin\_50 & maxpro\_50 \\
 & CCD & OACD & maximin & maxpro  \\
\hline
(Intercept)    & $0.3815^{***}$  & $0.3876^{***}$  & $0.3878^{***}$  & $0.3814^{***}$  \\
NP             & $-0.0134^{***}$ & $-0.0143^{***}$ & $-0.0042^{***}$ & $-0.0055^{*}$   \\
pMut           & $0.0020$        & $0.0028$        & $0.0043^{***}$  & $0.0045^{*}$    \\
pGBest         & $-0.0204^{***}$ & $-0.0224^{***}$ & $-0.0047^{***}$ & $-0.0082^{***}$ \\
pCR            & $-0.0022$       & $-0.0030$       & $0.0004$        & $0.0007$        \\
itermax        & $-0.0146^{***}$ & $-0.0152^{***}$ & $-0.0046^{***}$ & $-0.0021$       \\
NP$^2$          & $0.0119$        & $0.0080$        & $0.0022$        & $-0.0004$       \\
pMut$^2$        & $0.0150$        & $0.0174^{*}$    & $0.0083^{***}$  & $0.0104^{*}$    \\
pGBest$^2$      & $0.0083$        & $0.0192^{*}$    & $0.0035$        & $0.0159^{***}$  \\
pCR$^2$         & $0.0071$        & $0.0074$        & $-0.0008$       & $0.0049$        \\
itermax$^2$     & $0.0090$        & $-0.0081$       & $0.0011$        & $0.0057$        \\
NP:pMut        & $0.0058$        & $0.0064^{*}$    & $-0.0002$       & $-0.0037$       \\
NP:pGBest      & $-0.0041$       & $-0.0038$       & $0.0017$        & $0.0006$        \\
NP:pCR         & $0.0002$        & $-0.0007$       & $-0.0006$       & $-0.0002$       \\
NP:itermax     & $0.0049$        & $0.0033$        & $0.0023$        & $0.0052$        \\
pMut:pGBest    & $-0.0080^{*}$   & $-0.0093^{***}$ & $-0.0089^{***}$ & $-0.0139^{**}$  \\
pMut:pCR       & $0.0203^{***}$  & $0.0197^{***}$  & $0.0042^{**}$   & $0.0018$        \\
pMut:itermax   & $0.0032$        & $0.0025$        & $-0.0061^{***}$ & $0.0004$        \\
pGBest:pCR     & $0.0034$        & $0.0036$        & $0.0000$        & $0.0051$        \\
pGBest:itermax & $0.0057$        & $0.0068^{**}$   & $0.0076^{***}$  & $0.0058$        \\
pCR:itermax    & $0.0037$        & $0.0021$        & $0.0018$        & $0.0040$        \\
\hline
R$^2$          & $0.9034$        & $0.9235$        & $0.8970$        & $0.7297$        \\
Adj. R$^2$     & $0.8157$        & $0.8708$        & $0.8260$        & $0.5433$        \\
Num. obs.      & $43$            & $50$            & $50$            & $50$            \\
\hline
\multicolumn{5}{l}{\scriptsize{$^{***}p<0.001$; $^{**}p<0.01$; $^{*}p<0.05$}}
\end{tabular}
\caption{Comparison of second-order models from four training datasets}
\label{table:coefficients}
\end{center}
\end{table}

Table \ref{table:coefficients} shows the estimated coefficients of the second-order models based on the four different training datasets: CCD, OACD, maximin LHD, and maxpro LHD, for the target size $30 \times 3$.
Looking at the various models above, the model obtained using the maxpro LHD as the training data is the worst performing model. It has the lowest adjusted $R^2$ of only $0.54$. This model does not capture important main effects. For example, it indicates that the number of iterations ($itermax$) for optimization is not significant. Yet this is well known to be important. On the other hand, the model obtained by using OACD as the training dataset performs the best. It has an adjusted $R^2$ of $0.87$ and captures important main effects and interactions. In addition, CCD might be a little worse than OACD because of the fewer number of points $(43)$ used for training compared to the other models which used $50$ points.
The model from the CCD does not identify any of the quadratic effects to be significant while the other models do.

Looking at the corresponding p-values from the models obtained, it is evident that all five hyperparameters are important. The main effects of three hyperparameters ($NP$, $itermax$ and $pGBest$) are very significant, whereas the several interactions involving one of the other two hyperparameters ($pMut$ and $pCR$) are also very significant.
The probability of using the global best ($pGBest$) is quite important because its main effect is highly significant in all the models in Table \ref{table:coefficients}. It can also be inferred that the maximum number of iterations ($itermax$) and the population size ($NP$) are important. The linear models indicate that to minimize the objective function, it is ideal to use a larger $pGBest$, increase the population size ($NP$) and increase the number of iterations ($itermax$). The population size and the number of iterations could be constrained by the available budget.

With regards to interactions, all significant interaction terms involve either $pMut$ or $pGBest$. The interaction of $pMut$ and $pGBest$ is negative. On the other hand, the interaction between $pMut$ and $pCR$ is positive. As this is a minimization problem, this positive interaction indicates that one of the variables should be set at low  while the other high. We use contour plots to visualize the two parameters when the other three are held at their highest level.
%the two variables need to be either at their low levels or one has to be low while the other high. Both should not be at their high levels.

\iffalse %
\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{pdfs/interactions}
\caption{Interaction plots involving $pMut$ based on the $4^5$ FFD and target size $30\times3$.}
\label{interaction}
\end{figure}


%-------------------------------------------------------
To have a better understanding of the interactions, we examine some interaction plots from the full $4^5$  factorial, shown in Figure \ref{interaction}. Here, the number of population ($NP$), number of iterations ($itermax$) and the probability of using global best ($pGBest$) are held at their highest levels, i.e., 100, 1500 and 0.95, respectively. These results are consistent with the second-order models in Table \ref{table:coefficients}, where $NP$, $itermax$ and $pGBest$ are all negative and significant. Thus they should be set at the highest level to minimize the response values. These results are consistent for other target design sizes. For $pMut$ and $pCR$, their interaction is more complicated as it is positive and thus a further analysis is called for to determine the preferred levels to set these parameters. We use contour plots to visualize the two parameters when the other three are held at their highest level.

\fi %

\begin{figure}
\centering
\includegraphics[height=10cm, width=\textwidth]{pdfs/contours}
\caption{Contour plots of $pMut$ and $pCR$ while fixing other hyperparameters at the highest levels. Top row uses CCD as the training data; bottom row uses OACD as training data.}
\label{fig:contours}
\end{figure}

Figure \ref{fig:contours} shows the contour plots of $pMut$ and $pCR$ {based on the CCD and OACD training data set while fixing $NP=100$, $itermax=1500$ and $pGBest$=0.95. From the contour plot for target size $30\times 3$, we note that the response value could be minimized by taking values {along} the off diagonal, either} a smaller $pMut$ and larger $pCR$ or a larger $pMut$ and a smaller $pCR$. This is also the case for target sizes $50\times5$ and $70\times 7$. {It appears that different target sizes require different optimal settings for $pMut$ and $pCR$. Here, we take a simple approach to searching the optimal setting by varing $pMut$ from $0.05, 0.15, \ldots, 0.95$ and fixing $pCR=1-pMut$}. \revb{Using the grid search method, and a limited budget of 500 iterations, we look for the best combination of $pMut$ and $pCR$ that gives the best function value. This combination is then used for the remaining 1000 iterations to obtain the final objective value.}


%For each setting of $pMut$ and $pCR$, we run the DE algorithm $100$ times to construct $100$ designs while fixing $NP=100$, $itermax=1500$ and $pGBest=0.95$. The average response values are then calculated across these runs, allowing us to identify the optimal setting of $pMut$ and $pCR$ for each target size. This approach results in the folllowing optimal settings of $(pMut,~~ pCR)$: $(0.95, 0.05)$, $(0.25, 0.75)$, and $(0.15, 0.85)$ for target size $30\times3$, $50\times5$ and $70\times7$, respectively.
%{Table \ref{tab:DE_optim} shows the average and standard deviation (SD) of the $\phi(\cdot)$ values under these optimal hyperparameter settings.}


\revb{The optimal settings values are compared to values obtained using the two DE variants, DE1 and DE4, described by \textcite{stokes2024metaheuristic} and to the grid and random searches. DE1 uses only the global best solution (\( pGBest = 1 \)) while DE4 is a hybrid approach with \( pGBest = 0.5 \). Both DE1 and DE4 utilize fixed values of \( pMut = 0.1 \), \( pCR = 0.5 \), \( NP = 100 \) and \( itermax = 1500 \).  On the other hand, for grid search, the best settings {are chosen based on the $4^5$ FFD} while for the random search, a 1024-run random LHD is used instead. {Tables \ref{tab:grid_optim} and \ref{tab:lhd_optim} show the optimal hyperparameter settings and the corresponding performances from the grid search and the random search, respectively.}}

\revb{For comparison, %the optimal settings for \( pMut\) and \( pCR \), determined for each target size, are used while maintaining \( pGBest = 0.95 \). We
we refer to the approach used above as the DEoptim. %For the DEoptim, DE1 and DE4,
For all the methods, we set\( NP = 100 \) and \( itermax = 1500 \),
and the DE algorithm is executed 100 times to construct 100 designs.}

%%change include=true, eval=true to include the chunk
\begin{table}

\caption{\label{tab:grid_optim}Optimal hyperparameter settings and DE performace via grid search}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r}
\hline
  & NP & itermax & pMut & pCR & pGBest & Average $\phi(\cdot)$ & SD $\phi(\cdot)$\\
\hline
$30\times3$ & 100 & 1167 & 0.95 & 0.05 & 0.65 & 0.3863 & 0.0040\\
\hline
$50\times5$ & 100 & 1500 & 0.35 & 0.95 & 0.95 & 0.1688 & 0.0016\\
\hline
$70\times7$ & 100 & 1500 & 0.35 & 0.35 & 0.95 & 0.1063 & 0.0009\\
\hline
\end{tabular}
\end{table}\begin{table}

\caption{\label{tab:lhd_optim}Optimal hyperparameter settings and DE performace via random search}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r}
\hline
  & NP & itermax & pMut & pCR & pGBest & Average $\phi(\cdot)$ & SD $\phi(\cdot)$\\
\hline
$30\times3$ & 95 & 917 & 0.63 & 0.19 & 0.72 & 0.3878 & 0.0046\\
\hline
$50\times5$ & 98 & 1485 & 0.40 & 0.38 & 0.88 & 0.1688 & 0.0014\\
\hline
$70\times7$ & 84 & 1408 & 0.11 & 0.92 & 0.86 & 0.1064 & 0.0009\\
\hline
\end{tabular}
\end{table}

Figure \ref{fig:three graphs} {presents the performance of the DE algorithm under various hyperparameter settings, where {the DE algorithm was executed 100 times to construct 100 UPDs for each setting and} the vertical axis is the \( \phi(\cdot) \) values of the UPDs constructed. %The DEoptim method corresponds to the optimal settings in Table \ref{tab:DE_optim}.
The results demonstrate that DEoptim} produces significantly better UPDs, {especially for the large target size $70 \times 7$,} compared to the two variants, DE1 and DE4, and grid and random strategies, confirming the effectiveness of hyperparameter tuning.


\begin{figure}[!h]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{pdfs/boxplots2}
         \caption{$30\times 3$ as target}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{pdfs/boxplots3}
         \caption{$50\times 5$ as target}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{pdfs/boxplots4}
         \caption{$70\times 7$ as target}
     \end{subfigure}
        \caption{Performance of the DE algorithm under various setttings: DE1, DE4, DEoptim, Grid, Random}
        \label{fig:three graphs}
\end{figure}



% For a given target size and each method, we generate 100 UPDs using the respective optimal factor combination and compute their objective $\phi(\cdot)$ values.  Figure \ref{fig:three graphs} compares the objective values of the generated UPDs from the 5 methods.

\subsection*{Comparison between DE and SA in maximinLHD construction}

In the minimization of the maximinLHD criterion, Differential Evolution (DE), as used in the generation of Unipro designs, consistently outperforms Simulated Annealing (SA), the method employed in the SLHD package. The primary objective of minimizing this criterion is to construct a Latin Hypercube Design (LHD) with a lower maximin objective value, ensuring better space-filling properties. Empirical results demonstrate that DE achieves superior optimization performance by efficiently exploring and exploiting the search space. In contrast, SA, due to its reliance on a rigid cooling schedule, often struggles with premature convergence, leading to suboptimal designs. The adaptability of DE allows it to find better solutions across different problem dimensions and target sizes, whereas SAs probabilistic acceptance rule does not always yield optimal results.


One of the key differences between these two methods lies in their ability to balance exploration and exploitation. SA relies on a temperature-based acceptance rule to escape local minima, but its effectiveness is highly dependent on the cooling schedule. If the temperature decreases too quickly, the search stagnates in a poor local minimum; if it decreases too slowly, the method wastes computational resources. DE, on the other hand, maintains a dynamic balance between exploration and exploitation through its mutation and crossover mechanisms. By leveraging differential mutations, DE efficiently explores the search space while preserving diversity, ensuring that it does not get trapped in local minima. This adaptive approach enables DE to consistently find better designs, avoiding the stagnation issues that often arise in SA.

Another critical advantage of DE is its convergence speed and computational efficiency. SA requires a large number of iterations to refine its solutions due to its gradual cooling process, which makes it computationally expensive. While SA can eventually find good solutions, the process is slow and inefficient compared to DE. In contrast, DE converges more rapidly by efficiently distributing resources between exploration and refinement. Its ability to make guided mutations based on population differences accelerates the search, allowing it to produce high-quality solutions with fewer function evaluations. As a result, DE is a more efficient method for minimizing the maximinLHD criterion, especially in high-dimensional settings where computational cost is a concern.

In addition to efficiency, DE demonstrates superior robustness across different target sizes, including \(30 \times 3\), \(50 \times 5\), and \(70 \times 7\). To ensure valid cmparisons, the SA method with 15 random starts was employed. This gives better results than using just 1 random start and the timing is almost similar to that used within the DE approach. The results obtained for the three mentioned designs are as given in %\ref{fig:slhd}

\includegraphics{DE-014}


The method effectively adapts to varying problem complexities and consistently produces designs with better space-filling properties. SA, however, struggles as the dimensionality increases because its fixed parameter settings do not scale well. While DE dynamically adjusts its search strategy, SAs fixed cooling schedule limits its performance in larger design spaces, leading to suboptimal results. This makes DE a more reliable choice for optimizing LHDs across diverse problem settings.

The practical implications of DEs superior performance are significant. Given that DE produces better designs in a more computationally efficient manner, it is the preferred method for minimizing the maximinLHD criterion. Its ability to dynamically adapt hyperparameters and effectively navigate the search space makes it a powerful tool for applications that require high-quality space-filling designs, such as machine learning, computer experiments, and surrogate modeling. In contrast, SAs reliance on a rigid cooling schedule and slower convergence limits its practical utility. Overall, DE not only surpasses SA in terms of solution quality but also in computational efficiency, making it the superior method for generating optimal maximin designs.




\section{Conclusion}
This paper compared small designs in exploring the response surface of the DE algorithm hyperparameters. Five numerical hyperparameters were considered:  population size, the number of maximum iterations, probability of crossover, probability of mutation, and probability of using the global best for mutation. Various composite designs and space-filling designs for selecting combinations of these hyperparameters were also examined. The performance of a design was evaluated via building a second-order model, a kriging model and a heterogeneous GP model. The performance was measured in terms of testing RMSEs and correlation. The comparison was made based on data simulated using the uniform projection criterion. Under the settings considered, the comparison demonstrates that OACD and CCD are the better choices over space-filling designs for exploring the response surface of the DE algorithm hyperparameters. In addition, the second-order model is simple and works just as well as the kriging model and the heterogeneous GP model in this situation. The importance of tuning the DE algorithm is demonstrated and a simple strategy on determining optimal hyperparameter settings for  constructing UPDs with different target sizes is provided. An R package \texttt{UniPro} has been developed and is publicly available at \href{https://github.com/oonyambu/UniPro}{github.com/oonyambu/UniPro} for generating UPDs.

%\sout{While the primary goal of optimizing hyperparameters is to find an optimal hyperparameter combination that maximizes the overall performance of a learning algorithm, the paper additionally examines the impact of different design configurations on the effectiveness of hyperparameter tuning. The insights gained are subsequently used to select the best hyperparameters, which are then applied by the DE algorithm to construct UPDs.}
%\reva{\sout{The method is general and applicable for tuning hyperparameters in other optimization algotithms (revise properly; maybe state this in the introduction).}}
\pagebreak
\printbibliography

\clearpage
%\pagebreak
\section*{Appendix: Additional tables and figures for target sizes $50\times5$ and $70\times7$}



\begin{figure}[!ht]
\centering
\includegraphics[height=5in, width=6in]{pdfs/barplots2}
\caption{Comparison of RMSE with target size $50\times5$}
\label{barplots2}
\end{figure}


\begin{table}

\caption{\label{tab:tab2}Comparison of designs and model evaluations with target size $50 \times 5$}
\centering
\begin{tabular}[t]{|l|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\textbf{ }} & \multicolumn{6}{|c|}{\textbf{(a) Testing on the $3^5$ FFD}} & \multicolumn{6}{|c|}{\textbf{(b) Testing on the 243 LHD}} \\
\cline{2-7} \cline{8-13}
\multicolumn{1}{|c|}{ } & \multicolumn{3}{|c|}{correlation} & \multicolumn{3}{|c|}{RMSE} & \multicolumn{3}{|c|}{correlation} & \multicolumn{3}{|c|}{RMSE} \\
\cline{2-4} \cline{5-7} \cline{8-10} \cline{11-13}
Design & lm & km & hetGP & lm & km & hetGP & lm & km & hetGP & lm & km & hetGP\\
\hline
ccd3\_43 & 0.94 & 0.96 & 0.93 & 1.32 & 1.17 & 1.49 & 0.83 & 0.82 & 0.80 & 1.42 & 1.81 & 1.88\\
\hline
oacd3\_50 & 0.94 & 0.96 & 0.96 & 1.42 & 1.11 & 1.09 & 0.83 & 0.84 & 0.86 & 1.59 & 1.98 & 1.66\\
\hline
lhd\_50 & 0.83 & 0.85 & 0.83 & 2.35 & 2.19 & 2.26 & 0.89 & 0.92 & 0.93 & 1.18 & 0.97 & 0.92\\
\hline
maximin\_50 & 0.87 & 0.84 & 0.81 & 2.03 & 2.73 & 2.58 & 0.92 & 0.92 & 0.93 & 0.90 & 0.89 & 0.85\\
\hline
maxpro\_50 & 0.85 & 0.81 & 0.82 & 2.21 & 2.79 & 2.62 & 0.90 & 0.92 & 0.92 & 1.06 & 0.94 & 0.93\\
\hline


\multicolumn{1}{|c|}{\textbf{ }} & \multicolumn{6}{|c|}{\textbf{(c) Testing on the $3^5$ FFD+243 LHD}} & \multicolumn{6}{|c|}{\textbf{(d) Testing on the $4^5$ FFD}} \\
\cline{2-7} \cline{8-13}
\multicolumn{1}{|c|}{ } & \multicolumn{3}{|c|}{correlation} & \multicolumn{3}{|c|}{RMSE} & \multicolumn{3}{|c|}{correlation} & \multicolumn{3}{|c|}{RMSE} \\
\cline{2-4} \cline{5-7} \cline{8-10} \cline{11-13}
Design & lm & km & hetGP & lm & km & hetGP & lm & km & hetGP & lm & km & hetGP\\
\hline
ccd3\_43 & 0.92 & 0.92 & 0.89 & 1.37 & 1.53 & 1.70 & 0.93 & 0.93 & 0.92 & 1.32 & 1.40 & 1.50\\
\hline
oacd3\_50 & 0.91 & 0.92 & 0.93 & 1.51 & 1.61 & 1.41 & 0.92 & 0.94 & 0.94 & 1.45 & 1.44 & 1.30\\
\hline
lhd\_50 & 0.86 & 0.88 & 0.88 & 1.86 & 1.69 & 1.73 & 0.87 & 0.87 & 0.87 & 1.87 & 1.77 & 1.82\\
\hline
maximin\_50 & 0.90 & 0.85 & 0.85 & 1.57 & 2.03 & 1.92 & 0.90 & 0.87 & 0.86 & 1.61 & 2.08 & 1.98\\
\hline
maxpro\_50 & 0.88 & 0.83 & 0.85 & 1.73 & 2.08 & 1.97 & 0.88 & 0.85 & 0.86 & 1.75 & 2.11 & 1.99\\
\hline
\end{tabular}
\end{table}

\begin{figure}%[!ht]
\centering
\includegraphics[height=5in, width=6in]{pdfs/barplots3}
\caption{Comparison of RMSE with target size $70\times7$}
\label{barplots3}
\end{figure}



\begin{table}

\caption{\label{tab:tab3}Comparison of designs and model evaluations with target size $70 \times 7$}
\centering
\begin{tabular}[t]{|l|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\textbf{ }} & \multicolumn{6}{|c|}{\textbf{(a) Testing on the $3^5$ FFD}} & \multicolumn{6}{|c|}{\textbf{(b) Testing on the 243 LHD}} \\
\cline{2-7} \cline{8-13}
\multicolumn{1}{|c|}{ } & \multicolumn{3}{|c|}{correlation} & \multicolumn{3}{|c|}{RMSE} & \multicolumn{3}{|c|}{correlation} & \multicolumn{3}{|c|}{RMSE} \\
\cline{2-4} \cline{5-7} \cline{8-10} \cline{11-13}
Design & lm & km & hetGP & lm & km & hetGP & lm & km & hetGP & lm & km & hetGP\\
\hline
ccd3\_43 & 0.93 & 0.96 & 0.94 & 1.25 & 1.01 & 1.18 & 0.81 & 0.84 & 0.83 & 1.46 & 1.82 & 1.84\\
\hline
oacd3\_50 & 0.94 & 0.97 & 0.97 & 1.22 & 0.91 & 0.86 & 0.83 & 0.88 & 0.92 & 1.39 & 1.54 & 1.18\\
\hline
lhd\_50 & 0.83 & 0.84 & 0.82 & 2.15 & 2.28 & 2.18 & 0.94 & 0.95 & 0.95 & 0.75 & 0.69 & 0.72\\
\hline
maximin\_50 & 0.87 & 0.87 & 0.81 & 1.77 & 2.09 & 2.14 & 0.94 & 0.94 & 0.93 & 0.82 & 0.80 & 0.86\\
\hline
maxpro\_50 & 0.88 & 0.88 & 0.87 & 1.80 & 2.01 & 1.89 & 0.94 & 0.95 & 0.95 & 0.80 & 0.71 & 0.70\\
\hline


\multicolumn{1}{|c|}{\textbf{ }} & \multicolumn{6}{|c|}{\textbf{(c) Testing on the $3^5$ FFD+243 LHD}} & \multicolumn{6}{|c|}{\textbf{(d) Testing on the $4^5$ FFD}} \\
\cline{2-7} \cline{8-13}
\multicolumn{1}{|c|}{ } & \multicolumn{3}{|c|}{correlation} & \multicolumn{3}{|c|}{RMSE} & \multicolumn{3}{|c|}{correlation} & \multicolumn{3}{|c|}{RMSE} \\
\cline{2-4} \cline{5-7} \cline{8-10} \cline{11-13}
Design & lm & km & hetGP & lm & km & hetGP & lm & km & hetGP & lm & km & hetGP\\
\hline
ccd3\_43 & 0.90 & 0.91 & 0.90 & 1.36 & 1.47 & 1.54 & 0.92 & 0.94 & 0.93 & 1.28 & 1.26 & 1.33\\
\hline
oacd3\_50 & 0.91 & 0.93 & 0.95 & 1.30 & 1.27 & 1.03 & 0.92 & 0.94 & 0.95 & 1.29 & 1.21 & 1.06\\
\hline
lhd\_50 & 0.87 & 0.87 & 0.87 & 1.61 & 1.69 & 1.62 & 0.86 & 0.87 & 0.86 & 1.81 & 1.85 & 1.79\\
\hline
maximin\_50 & 0.90 & 0.88 & 0.86 & 1.38 & 1.58 & 1.63 & 0.89 & 0.89 & 0.86 & 1.49 & 1.67 & 1.75\\
\hline
maxpro\_50 & 0.90 & 0.89 & 0.90 & 1.39 & 1.51 & 1.43 & 0.90 & 0.90 & 0.90 & 1.49 & 1.58 & 1.51\\
\hline
\end{tabular}
\end{table}




\end{document}
